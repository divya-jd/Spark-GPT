{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation_Dashboard.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Mock predictions vs ground truth\n",
    "data = pd.DataFrame({\n",
    "    \"query\": [\"What is RAG?\", \"Define LangChain\", \"Use of Spark in AI\"],\n",
    "    \"pred\": [\"Retrieval-Augmented Generation combines retrieval and LLM.\", \n",
    "             \"LangChain orchestrates LLM workflows.\", \n",
    "             \"Spark enables distributed data pipelines for AI.\"],\n",
    "    \"true\": [\"RAG augments language models with retrieval.\", \n",
    "             \"LangChain builds contextual LLM applications.\", \n",
    "             \"Spark handles distributed compute for ML tasks.\"]\n",
    "})\n",
    "\n",
    "# BLEU computation\n",
    "data[\"BLEU\"] = data.apply(lambda r: sentence_bleu([r[\"true\"].split()], r[\"pred\"].split()), axis=1)\n",
    "avg_bleu = data[\"BLEU\"].mean()\n",
    "\n",
    "# Latency simulation\n",
    "data[\"Latency\"] = np.random.uniform(0.2, 0.6, size=len(data))\n",
    "\n",
    "# Visualize\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(data[\"query\"], data[\"BLEU\"], color='green')\n",
    "ax1.set_ylabel(\"BLEU Score\", color='green')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(data[\"query\"], data[\"Latency\"], color='orange', marker='o')\n",
    "ax2.set_ylabel(\"Latency (s)\", color='orange')\n",
    "plt.title(f\"RAG Evaluation Dashboard | Avg BLEU: {avg_bleu:.2f}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
